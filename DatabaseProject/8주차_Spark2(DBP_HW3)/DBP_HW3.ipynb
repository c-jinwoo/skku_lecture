{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IjJsBUv2Ry7"
      },
      "source": [
        "# Database Project (SWE3033) (Fall 2023)\n",
        "# HW3 (100pts, Due date: 11/3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgESR0--75pO"
      },
      "source": [
        "**Instruction:** In this homework, we provide you with a jupyter notebook file (DBP_HW3.ipynb). You should follow the instructions in these documents carefully."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mrn9f8dQ75-h"
      },
      "source": [
        "**Submit two files as follows**:\n",
        "- `DBP_HW3_StudentID.zip`\n",
        "\t- `DBP_HW3_StudentID.ipynb`\n",
        "\t- `DBP_HW3_StudentID.pdf`\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgteHPoSOc8G"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fFfFeuH2eoL"
      },
      "source": [
        "### Import required library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TcyrM0rSa9l"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SQLContext, SparkSession\n",
        "from pyspark.sql import Row\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "spark = SparkSession(sc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JowpPl7jRl1H"
      },
      "source": [
        "# 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYbKMobZRqMl"
      },
      "source": [
        "#### (a) Create a DataFrame with the given data and display the generated DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNlfIRGjRlmR"
      },
      "outputs": [],
      "source": [
        "# ============= EDIT HERE =============\n",
        "df = None\n",
        "\n",
        "# =====================================\n",
        "\n",
        "df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqV2yo6ERyQs"
      },
      "source": [
        "#### (b) After adding two laptops to the manufacturing process, find the number of products for each model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofmYN5DIAlJV"
      },
      "outputs": [],
      "source": [
        "# ============= EDIT HERE =============\n",
        "new_df = None\n",
        "\n",
        "# =====================================\n",
        "new_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAE5hw5MAll7"
      },
      "source": [
        "#### (c) Group the data in the joined DataFrame by ‘phases’ and count the number of data for each phase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_SvXHUPRlkD"
      },
      "outputs": [],
      "source": [
        "# ============= EDIT HERE =============\n",
        "group_df = None\n",
        "\n",
        "# =====================================\n",
        "group_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpqsmhXJR59A"
      },
      "source": [
        "## 2.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hq_B3f90R66t"
      },
      "source": [
        "#### (a) Create a DataFrame for the two given data and join Data 1 with Data 2 using an inner join based on the 'serial' column. (left side: Data 2, right side: Data 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkC6i93MGiDB"
      },
      "outputs": [],
      "source": [
        "# ============= EDIT HERE =============\n",
        "\n",
        "joined_df = None\n",
        "\n",
        "# =====================================\n",
        "\n",
        "joined_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhKT-7xjR_LC"
      },
      "source": [
        "#### (b) Use an SQL query to select the data from the joined DataFrame where the ‘due_date’ is on or after ‘2023-10-25’. And briefly explain the method you used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dS5Dlp0umXRh"
      },
      "outputs": [],
      "source": [
        "# ============= EDIT HERE =============\n",
        "sql_df = None\n",
        "\n",
        "\n",
        "# =====================================\n",
        "\n",
        "sql_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHMayamBTuFr"
      },
      "source": [
        "## 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eb__FgHnly25"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ClassificationPractice\").getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kLZmq2jpxmU"
      },
      "source": [
        "###  (a) Load the provided dataset, convert it into a DataFrame, and show it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48tA3y90XD2g"
      },
      "outputs": [],
      "source": [
        "def create_dataframe(data_path):\n",
        "\n",
        "    fasion_mnist_data = spark.read.option('header', 'true').option('inferSchema', 'true').csv(data_path)\n",
        "\n",
        "    col_lst = ['pixel'+str(i+1) for i in range(784)]\n",
        "\n",
        "    # ============= EDIT HERE =============\n",
        "\n",
        "    # Instruction 1: Assemble the features into a vector column and name the column \"features\"\n",
        "\n",
        "\n",
        "    # Instruction 2: Rename the target column to \"label\"\n",
        "    fasion_mnist_data = None\n",
        "\n",
        "    # =====================================\n",
        "\n",
        "    return fasion_mnist_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ry0m8PZW6QO"
      },
      "outputs": [],
      "source": [
        "# ============= EDIT HERE =============\n",
        "# OPTION: You can also change the data path.\n",
        "train_data_path = 'fashion-mnist_train.csv'\n",
        "test_data_path = 'fashion-mnist_test.csv'\n",
        "# =====================================\n",
        "train_data = create_dataframe(train_data_path)\n",
        "test_data = create_dataframe(test_data_path)\n",
        "\n",
        "train_data.show()\n",
        "test_data.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OakFNEVS0YUL"
      },
      "outputs": [],
      "source": [
        "# Multi-class classification evaluator\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uN2YagmCXehd"
      },
      "source": [
        "### Logistic Regression\n",
        "Reference: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.LogisticRegression.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGv0xOMXWyU3"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# Training and Test\n",
        "# ============= EDIT HERE =============\n",
        "lr_model = None\n",
        "lr_preds = None\n",
        "lr_accuracy = None\n",
        "\n",
        "# =====================================\n",
        "\n",
        "print(f\"Accuracy: {lr_accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eU6BP1H_XkSI"
      },
      "source": [
        "### Decision Tree\n",
        "Reference: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.DecisionTreeClassifier.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgt1-ajCXlp9"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "\n",
        "# On model declaration, fix seed, maxDepth,  to the following values\n",
        "seed = 2023\n",
        "\n",
        "# ============= EDIT HERE =============\n",
        "\n",
        "dt_model = None\n",
        "dt_preds = None\n",
        "dt_accuracy = None\n",
        "\n",
        "# =====================================\n",
        "\n",
        "print(f\"Accuracy: {dt_accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WpdhtJeXibN"
      },
      "source": [
        "### Random Forest\n",
        "Reference: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.RandomForestClassifier.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7B9VhzXXj87"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "\n",
        "# On model declaration, fix seed to the following values\n",
        "seed = 2023\n",
        "\n",
        "# ============= EDIT HERE =============\n",
        "rfc_model = None\n",
        "rfc_preds = None\n",
        "rfc_accuracy = None\n",
        "\n",
        "# =====================================\n",
        "\n",
        "print(f\"Accuracy: {rfc_accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcKIkr5d5CeH"
      },
      "source": [
        "# Make sure to save both the code and output values prior to submission."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
